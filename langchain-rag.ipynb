{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3f8936-82da-499b-aece-ab84ecb272c7",
   "metadata": {},
   "source": [
    "# Simple LangChain RAG.  \n",
    "\n",
    "Following this tutorial\n",
    "[Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "\n",
    "## Improt Environement Variables from File\n",
    "Need to have a .env file with OPENAI_API_KEY and a LANGSMITH_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91261279-dea6-4aba-a6ad-82c2b20f3dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import os\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "env = Env()\n",
    "env.read_env(\"/Users/geoffreysmalling/development/langchain/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c4419-19a9-47e8-b1c8-d3f72a39f894",
   "metadata": {},
   "source": [
    "## connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ddffa-93d6-407c-a55a-460743e967e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=env.str(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-0125\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = env.str(\"LANGSMITH_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2d5ff-24d8-4ad8-a0a2-2f8e891c76c8",
   "metadata": {},
   "source": [
    "## Test llm connection and get answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cfb0f-b755-44f1-94c6-87d0e6e901a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908836e2-2b4c-4d19-b744-b116d21c072c",
   "metadata": {},
   "source": [
    "# load, chuck, and index contents of the source\n",
    "use the WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d00df0-a19d-4edd-b92e-f326a60a7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(\"type of docs object: \" + str(type(docs)))\n",
    "print(\"number of docs: \" + str(len(docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0ca8c-813c-4795-990a-c05a3c09c7ff",
   "metadata": {},
   "source": [
    "## split the docs into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce6044-770d-4f26-97cf-44bf8fa1776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"type of splits object: \" + str(type(splits)))\n",
    "print(\"number of splits: \" + str(len(splits)))\n",
    "print(\"metadata example: \" + str(splits[10].metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0e8f2-cd3b-491f-9ae2-300940490b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## store in a vector store using openAI Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32022b1f-e035-4f9f-8f38-66f11f857df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a02c79-90e9-4432-af4c-3932969a461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type of vectorstore object: \" + str(type(vectorstore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8d2a7-aa37-4860-a41d-021a7cc1520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a retriever and pull a prompt from the langsmith hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09be49b-303c-4ad9-8d5a-405a691edc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(retrieved_docs[0].metadata['source'])\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2670a-e9e1-436e-abdb-71709ea9e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a method to break each chunk of docs into a new paragraph for the prompt context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840ec4e-06e9-43b5-82f9-387e904cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    context =  \"\\n\\n\".join(doc.page_content + \" source: \" + doc.metadata['source'] + \" \"   for doc in docs)\n",
    "    return context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d8679-4f3e-42de-b84e-96b7144e952c",
   "metadata": {},
   "source": [
    "## Build the LangChain\n",
    "- User the retriever to get context docs and then join them with format docs\n",
    "- Get the question to the llm via RunnablePassthrough\n",
    "- pass context and question to the prompt\n",
    "- pass prompt to llm\n",
    "- parse results to a string, vs a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500bb19-9f39-466d-82c4-9542a2641189",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    #| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e00076-3eee-4fa8-97ee-c687b7be5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f6bf0-a922-4487-af43-8ab1e1862d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ea93c-37b9-424f-a2ee-d5d1423c76c9",
   "metadata": {},
   "source": [
    "# adding chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727e93a-5928-4dc3-9624-82efe0182643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9bf52-24cf-424e-94ae-9c67ee5422c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b15e5f-7af4-4cd7-a01c-fac1987744b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818cabb-f063-4317-b678-57becfde5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in ai_msg_2[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e95e6-054b-4804-a6b0-49a9b8deb9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
