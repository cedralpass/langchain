{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3f8936-82da-499b-aece-ab84ecb272c7",
   "metadata": {},
   "source": [
    "# Understanding Runable\n",
    "\n",
    "Runnable is the framework that controls the piping of invokes. \n",
    "langchain_core.runnables.base.RunnableParallel runs in parallel\n",
    "langchain_core.runnables.base.RunnableSequence runs in sequence\n",
    "RunnableLambda is a lambda function to wrap functions in.\n",
    "\n",
    "## Improt Environement Variables from File\n",
    "Need to have a .env file with OPENAI_API_KEY and a LANGSMITH_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91261279-dea6-4aba-a6ad-82c2b20f3dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import os\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda,RunnableParallel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = Env()\n",
    "env.read_env(\"/Users/geoffreysmalling/development/langchain/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f2c13-8b04-46f7-a2ef-12704f6b7903",
   "metadata": {},
   "source": [
    "# simple example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc8827-3d3e-488b-8e35-bef9987c0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "chain.invoke(\"cycling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def mul_two(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "def mul_three(x: int) -> int:\n",
    "    return x * 3\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(mul_two)\n",
    "runnable_3 = RunnableLambda(mul_three)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc55b78-3356-4a1f-9f73-18b27a7f8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_parallel_2_3 = RunnableParallel({\"mul_two\": runnable_2,\n",
    "                                          \"mul_three\": runnable_3}) \n",
    "\n",
    "sequence = runnable_1 |runnable_parallel_2_3\n",
    "\n",
    "print(runnable_1.invoke(1))\n",
    "\n",
    "print(sequence.invoke(1))\n",
    "await sequence.ainvoke(1)\n",
    "\n",
    "print(sequence.batch([1, 2, 3]))\n",
    "await sequence.abatch([1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40c4f3-fc9f-41da-b0ea-27ab188dd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = runnable_1 | RunnableParallel(\n",
    "    mul_two=runnable_2,\n",
    "    mul_three=runnable_3,\n",
    ")\n",
    "\n",
    "sequence.invoke(1)\n",
    "await sequence.ainvoke(1)\n",
    "\n",
    "sequence.batch([1, 2, 3])\n",
    "await sequence.abatch([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1021749-5e9e-4fdf-b870-fce573161b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n",
    "    \"mul_two\": runnable_2,\n",
    "    \"mul_three\": runnable_3,\n",
    "}\n",
    "\n",
    "sequence.invoke(1)\n",
    "await sequence.ainvoke(1)\n",
    "\n",
    "sequence.batch([1, 2, 3])\n",
    "await sequence.abatch([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0b3c9-9fe5-46cf-9f21-d578758b6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "    | model\n",
    ")\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "    | model\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "# Display stream\n",
    "output = {key: \"\" for key, _ in runnable.output_schema()}\n",
    "for chunk in runnable.stream({\"topic\": \"bear\"}):\n",
    "    for key in chunk:\n",
    "        output[key] = output[key] + chunk[key].content\n",
    "    print(output)  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3a08c-da18-46c7-a8a1-baa9b33456bc",
   "metadata": {},
   "source": [
    "# RunnableRunnablePassthrough \n",
    "\n",
    "Runnable to passthrough inputs unchanged or with additional keys.\n",
    "\n",
    "This runnable behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dict.\n",
    "\n",
    "The examples below demonstrate this Runnable works using a few simple chains. The chains rely on simple lambdas to make the examples easy to execute and experiment with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4706dc-e102-46b7-81ec-8e7a8c2c0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}\n",
    "\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125789ce-8bbe-4d42-ad22-4cf17f80e489",
   "metadata": {},
   "source": [
    "## adding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abeae7-e997-4186-9dd1-b38168011785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return prompt\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")\n",
    "\n",
    "runnable.invoke('hello')\n",
    "# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20147f7d-b379-4356-945e-81908f4a1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",api_key=env.str(\"OPENAI_API_KEY\"))\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(retriever.invoke(\"where did harrison work?\"))\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "print(setup_and_retrieval.invoke(\"where did harrison work?\"))\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7503d-5b31-4d91-811d-de63367c4615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ad46e-b545-44b6-b2d9-70df41e142a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
